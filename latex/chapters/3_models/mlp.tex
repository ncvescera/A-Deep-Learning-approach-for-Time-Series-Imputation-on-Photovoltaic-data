\section{MLP}\label{sec:mlpbaseline}
In this section, we will introduce the first model, which we will refer to as the
baseline model, based on a Multi-Layer Perceptron.
We will analyze its architecture, the training phase, and its final performance.

%In questa sezione andremo ad introdurre il primo modello a cui faremo
%riferiento come baseline model, basato su MultiLayer Perceptron.
%Analizzeremo la sua architettura, la 
%fase di training e le sue performance finali.

\subsection{Architecture}
This neural network is designed to predict the instantaneous energy output over
a period of exactly 2 days.
To do this, it requires input data that includes the performance of the system
(features selected during the Preprocessing phase, see Chapter \ref{chap:datapreprocessing}) for exactly one day
before and one day after the period in question.
This enables it to understand how the system is performing and,
consequently, provide the energy output trend.
The model consists of 6 main layers:

%Questa rete neurale è progettata per prevedere l'andamento dell'energia istantanea prodotta durante un periodo di un buco di esattamente 2 giorni.
%Per farlo ha bisogno di avere come input l'andamento dell'impianto (features selezionate nella fase di preprocessing) di esattamente un giorno prima del
%buco e di un giorno dopo. In questo modo può riuscire a comprendere come l'impianto sta performando e quindi restituire l'andamento dell'energia.
%Il modello è formato da 6 livelli principali:

\begin{figure}[H]
	\begin{minipage}{0.6\textwidth}
		\begin{itemize}
			\item \textbf{Input layer}: This is the first layer of the network.
			      It takes two input tensors: \textit{before} and \textit{after}.
			      These tensors represent the day before and the day after the specific period we want to
			      predict. They have the shape \verb|[BATCH_SIZE, 96, 33]|, where 96 is the number
			      of timestamps in our dataset that make up one day, and 33 represents the
			      features obtained from the Data Preprocessing phase.
			      These tensors are then flattened and concatenated to be passed to the subsequent layer.
			      The output of this layer goes through a Batch Normalization layer, and the
			      Rectified Linear Unit (ReLU) activation function is used.

			      % è il primo livello della rete, prende in input 2 tensori: \textit{before} e \textit{after}. Questi stanno ad indicare rispettivamente il giorno prima e quello dopo del buco che vogliamo chiudere. Avranno la forma \verb|[BATCH_SIZE, 96, 33]|, dove \verb|96| è il numero di timestamp del nostro dataset che formano un giorno e \verb|33| sono le feature ottenute dalla fase di preprocessing dei dati. A questi verrà poi applicata un'operazione di Flatten ed infine concatenati per poter essere poi passati al layer successio.
			      %L'output di questo layer passa per un livello di Batch Normalization e viene utilizzata la ReLu come funzione di attivazione. 

			\item \textbf{Hidden layers}: In total, there are 4 layers, each of which takes the
			      output of the previous layer as input and reduces the number of neurons by half.
			      Batch Normalization is applied to the result, and the Rectified Linear Unit (ReLU)
			      is used as the activation function.

			      %in totale 4, ognuno prende in input il risultato del layer precedente e va a dimezzare il numero dei neuroni. Al risultato viene applicata un'operazione di Batch Normalization e utilizzata la ReLu come funzione di attivazione.

			\item \textbf{Output layer}: The final layer of our network, it takes the result from the
			      previous layer and outputs the value of the Instantaneous Energy Produced
			      during the specific period.
			      It produces a tensor with a shape of \verb|[BATCH_SIZE, 192, 1]|.
			      The SoftPlus function is used as the final activation function.

			      %ultimo layer della nostra rete, dal risultato del layer precedente riporta come output il valore dell'Energia Istantanea Prodotta durante il buco, un tensore di forma \verb|[BATCH_SIZE, 192, 1]|. Viene utilizzata la SoftPlus come funzione di attivazione finale.
		\end{itemize}
	\end{minipage}%
	\hspace{0.5cm}
	\begin{minipage}{0.4\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{chapters/3_models/imgs/ufcnmodel.png}
		\caption{Beseline Model architecture visualization.}\label{fig:baselinemodelarch}
	\end{minipage}
\end{figure}

\begin{minipage}[t]{0.5\textwidth}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\draw[->] (-3,0) -- (3,0) node[right] {$x$};
			\draw[->] (0,-1) -- (0,4) node[above] {$y$};
			\draw[dotted] (-3,-1) grid (3,4);
			\draw[color=blue, domain=-3:0] plot[id=logistic] function{0};
			\draw[color=blue, domain=0:3] plot[id=logistic] function{x};
		\end{tikzpicture}
		\caption{Rectified Linear Unit function. $ReLu(x) = \max(0, x)$}
		\label{fig:relu}
	\end{figure}
\end{minipage}%
\hspace{.5cm}
\begin{minipage}[t]{0.5\textwidth}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\draw[->] (-3,0) -- (3,0) node[right] {$x$};
			\draw[->] (0,-1) -- (0,4) node[above] {$y$};
			\draw[dotted] (-3,-1) grid (3,4);
			\draw[color=blue, domain=-3:3] plot[id=logistic] function{log(1+exp(x))};
		\end{tikzpicture}
		\caption{SoftPlus function. $SoftPlus(x) = \frac{1}{\beta} \log(1+e^{\beta x})$}
		\label{fig:softplus}
	\end{figure}
\end{minipage}

\subsection{Training}
The model was trained by artificially creating gaps of two days in length within the
training dataset.
These gaps were passed to the network in the format described earlier,
and the output result was compared to the actual instantaneous energy produced during the gap.
An \textit{Early Stopping} procedure was implemented to prevent training from continuing if
the model was not improving its performance.
A procedure, called \textit{Save Best}, has also been integrated,
which saves the model to a file whenever the Validation Loss improves.
The validation dataset was applied in this phase to assess the learning progress at the end
of each epoch.
A normalization procedure of the area was applied to the model's output in relation to
that of the gap to ensure that the output starts exactly from the last value of the
instantaneous energy produced \textit{before} and ends exactly with the first value
of the energy produced \textit{after}. Adam was used as the optimizer,
and L1Loss (Equation~\ref{eq:l1loss}) served as the loss function.
We chose to set the batch size parameter to 10, the learning rate $\lambda$ to 0.01,
a maximum of 100 epochs, and a patience value of 20 for Early Stopping.

%Il modello è stato allenato creando, in modo artificiale, buchi di lunghezza
%pari a due giorni all'interno del dataset di training, passati alla rete nel formato
%descritto precedentemente e confrontato il risultato in output con l'effettiva energia istantanea
%prodotta del buco. \'{E} stata implementata una procedura di \textit{Early Stopping} per evitare
%di continuare l'allenamento anche se il modello non sta migliorando le sue performance. 
%\'{E} stata anche integrata una procedura, chiamata \textit{Save Best},
%che salva il modello su file ogni qualvolta la validation loss migliori.
%Il dataset di validation è stato applicato in questa fase per verificare lo stato di
%apprendimento alla fine di ogni epoca. All'output del modello è stata applicata una 
%procedura di normalizzazione dell'area rispetto a quella del buco per far si che la predizione
%parta esattamente dall'ultimo valore dell' energia istantanea prodotta di \textit{before} e
%termini esattamente con il primo valore di \textit{after}.
%\'{E} stato impiegato Adam come ottimizzatore e la L1Loss (Equation \ref{eq:l1loss}) come loss function.
%Abbiamo scelto di impostare il parametro batch size a 10, learning rate $\lambda$ a 0.01, 100
%come numero massimo di epoche e 20 come patience per l'Early Stopping.

\begin{gather}\label{eq:l1loss}
	L = \{l_1,\dots,l_N\}^\top, \quad
	l_n = \left| x_n - y_n \right|,\\
	\ell(x,y) = \operatorname{mean}(L)
\end{gather}

\begin{table}[H]
	\begin{center}
		\begin{tabular}[c]{l|l}

			\textbf{Total Parameters (\#)}     & 26543400 \\
			\textbf{Trainable Parameters (\#)} & 26543400 \\
			\textbf{Training Duration (s)}     & 24.0     \\
			\textbf{Model Size (MB)}           & 101.3
		\end{tabular}
	\end{center}
	\caption{Baseline Model specification.}\label{tab:ufcnspecs}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{chapters/3_models/imgs/ufnc/ufnctraining.png}
	\caption{The chart displays the loss progression during the training phase. The blue line represents the Training Loss, while the orange line represents the Validation Loss.}
	\label{fig:ufcntraining}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{chapters/3_models/imgs/ufnc/ufcncpusage.png}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{chapters/3_models/imgs/ufnc/ufcnmem.png}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{chapters/3_models/imgs/ufnc/ufcnusagevera.png}
	\end{subfigure}\\
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{chapters/3_models/imgs/ufnc/ufncusageperc.png}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{chapters/3_models/imgs/ufnc/ufncusagew.png}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{chapters/3_models/imgs/ufnc/ufcnmemram.png}
	\end{subfigure}\\
	\caption{System resources utilized during the Training phase.}
	\label{fig:ufcnsysusage}
\end{figure}

\begin{algorithm}[H]
	\caption{MLP model Training Algorithm}\label{alg:ufcntraining}
	\begin{algorithmic}
		\Require train/validation datasets; Baseline Neural Network Model

		\State Batch Size $\gets$ 10
		\State Learning Rate $\lambda \gets$ 0.01
		\State Epochs $\gets$ 100
		\State Patience $\gets$ 20
		\State loss $\gets$ L1Loss()
		\State Optimizer $\gets$ Adam Optimizer
		\State
		\For{\textbf{each} epoch \textbf{in} epochs}
		\For{\textbf{each} (batch\_id, before, after, target) \textbf{in} train.next\_batch()}

		\State train\_prediction $\gets$ model(before, after) \Comment{Model inference}
		\State train\_prediction $\gets \frac{\text{train\_prediction} \cdot \sum\text{train\_prediction}}{\sum target}$ \Comment{Area normalization}
		\State train\_loss $\gets$ loss(train\_prediction, target)
		\State Optimizer step
		\State Back Propagation
		\EndFor
		\State stop computing gradient
		\For{\textbf{each} (batch\_id, vbefore, vafter, vtarget) \textbf{in} validation.next\_batch()}
		\State val\_prediction $\gets$ model(vbefore, vafter) \Comment{Model inference}
		\State val\_prediction $\gets \frac{\text{val\_prediction} \cdot \sum\text{val\_prediction}}{\sum vtarget}$ \Comment{Area normalization}
		\State val\_loss $\gets$ loss(val\_prediction, vtarget)
		\EndFor

		\State check for Early Stopping
		\State check for Save Best Result
		\State start computing gradient
		\EndFor
	\end{algorithmic}
\end{algorithm}

\subsection{Evaluation}
From the training phase graph shown in Figure~\ref{fig:ufcntraining},
we can see that it has concluded successfully because the two curves
(training loss and validation loss) do not diverge but remain at a
relatively constant distance from each other.
This also suggests that there is no overfitting.
However, the values of the two loss functions do not appear to decrease
significantly as the epochs progress, indicating that the model may
not have generalized well.

%Dal grafico della fase di training mostrato in Figura~\ref{fig:ufcntraining}
%possiamo evicnere come questa sia terminata con successo dal
%fatto che le due curve (training loss e validation loss) non divergono
%ma rimangono ad una distanza abbastanza costante e possiamo
%anche capire che non ci sia presenza di overfitting.
%I valori delle due loss però non sembrano scendere col passare
%delle epoche ma rimangono pressochè costanti, indice che il modello
%potrebbe non essere riuscito a generalizzare bene.

From the graphs in Figure~\ref{fig:ufcnsysusage}, it is apparent that the
machine on which this model was trained was relatively underutilized.
In particular, it's worth noting that GPU utilization did not exceed 30\%,
and GPU memory was barely utilized, staying at around 20\%.
This suggests that the model can be trained on less powerful machines as well.
The model is also relatively lightweight in terms of memory,
with just 100 MB (see Table~\ref{tab:ufcnspecs}), and the training time
did not exceed 30 seconds.
Inference time is extremely fast, taking less than a second.

%Dai grafici in Figura~\ref{fig:ufcnsysusage} possiamo notare come
%la macchina su cui è stato addestrato questo modello sia stata
%sfruttata relativamente poco, in particolare facciamo notare come
%l'utilizzo della GPU non ha superato il 30\% e la sua memoria è stata
%a malapena occupata per il 20\%. Questo può portarci ad affermare che
%il modello può essere anche addestrato su macchine molto meno performanti
%della nostra. Il modello risulta essere anche relativamente leggero sia
%in termini di memoria, soli 100 MB (vedi Tabella~\ref{tab:ufcnspecs}) sia
%per il tempo di allenamento che non ha superato i 30 secondi. Il tempo di
%inferenza risulta estremamente veloce non superando il secondo.

\begin{figure}[H]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{chapters/3_models/imgs/ufnc/eval/ufcpred2-4.png}
		\caption{}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{chapters/3_models/imgs/ufnc/eval/ufcpred6-4.png}
		\caption{}
	\end{subfigure}
	\caption{The figure displays two predictions made by the model, where you can observe the network's output (in red) and the ground truth (in blue). The model was tested on two gaps, each covering a period of 2 days. The first gap (a) spans from 02-04-2023 to 04-04-2023, and the second gap (b) spans from 06-04-2023 to 06-08-2023.}

	%La figura riporta due predizioni del modello dove possiamo apprezzare l'output della rete (in rosso) e la ground trought (in blu). Al modello sono stati fatti chiudere due buchi di 2 giorni ciuascuno che coprono rispettivamente il periodo dal 02-04-2023 al 04-04-2023 e dal 06-04-2023 al 06-08-2023.}
	\label{fig:ufcnevalbelli}
\end{figure}

Analyzing the graphs presented in Figure~\ref{fig:ufcnevalbelli},
we can confirm the previous suspicion that the model did not learn well how
to predict the trend of instant energy production from the plant.
Plot (a) clearly illustrates how the model's prediction is very timid
and fails to grasp the possibility that some days may be more productive
than others.
In graph (b), we can see how the model almost approximates the second
day but makes a significant mistake on the first,
failing to understand the potential for production spikes during the day.
It's also evident that the model struggles to handle the
day/night cycle, often ending production much earlier than it should.
However, it consistently predicts zero energy production
during the night, which is a positive aspect.

%Analizzando i grafici mostrati in Figura~\ref{fig:ufcnevalbelli} possiamo
%confermare il sospetto precedente, il modello non è riuscito ad apprendere
%bene come predirre l'andamento della curva dell'energia istantanea prodotta
%dall'impianto. Il plot (a) mostra molto bene come la predizione del modello sia
%molto timida e non riesce minimamente a comprendere la possibilità che alcuni
%giorni possono essere più produttivi di altri. Mentre nel grafico (b) possiamo
%vedere come il modello riesce quasi ad approssimare il secondo giorno, ma sbagliando
%notevolmente nel primo non risucendo a comprendere la possibilità di picchi di produzione durante la giornata.
%Si evince inoltre come questo ha difficoltà nel gestire il ciclo giorno/notte
%dato che molto spesso fa terminare la produzione molto prima di quando dovrebbe.
%Ottimo però il fatto che la produzione notturna risulti sempre nulla.

\begin{figure}[H]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{chapters/3_models/imgs/ufnc/eval/ufcpred3-4.png}
		\caption{}
	\end{subfigure}\\
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{chapters/3_models/imgs/ufnc/eval/ufcpred5-4.png}
		\caption{}
	\end{subfigure}
	\caption{The figure displays two model predictions (in red) for two-day gaps and their respective ground truth (in blue). (a) corresponds to a gap from 03-04-2023 to 04-04-2023, while (b) covers the period from 05-04-2023 to 06-04-2023.}
	%In figura sono mostrati due predizioni del modello (in rosso) relative a buchi di due giorni e le rispettive ground truth (in blu). (a) fa riferimento ad un buco che va dal 03-04-2023 fino al 04-04-2023, mentre (b) copre il periodo che va dal 05-04-2023 fino al 06-04-2023.}
	\label{fig:ufcnevalbrutti}
\end{figure}

Figure~\ref{fig:ufcnevalbrutti} highlights the significant limitations
of the model, demonstrating how it fails to identify potential peak
values in production and how the predicted day curves are consistently similar
to each other.
It's also important to note that the model can only work with
gaps of a fixed size, and if a gap of a larger size is encountered,
it would be necessary to retrain the model.

%La Figura~\ref{fig:ufcnevalbrutti} mette in risalto i notevoli limiti del modello
%mostrando come non riesca minimamente ad individuare possibili valori di picco della 
%produzione e come le curve dei giorni predetti siano sempre molto simili tra di loro.
%\'{E} anche opportuno segnalare il fatto che il modello possa solo lavorare con
%buchi di dimensione sempre fissa e, qualora si presentasse un buco di più gradi 
%dimensioni sia necessario dover riallenare nuovamente il modello.